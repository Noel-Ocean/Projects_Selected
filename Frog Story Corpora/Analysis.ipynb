{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download es_core_web_trf\n",
    "# nltk.download()\n",
    "import spacy \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from chamd import ChatReader\n",
    "reader = ChatReader()\n",
    "\n",
    "# Import the Self-scripted .py for task-specific functions\n",
    "import Analysis \n",
    "from Analysis import raw_chat_to_dataframe\n",
    "from Analysis import summary_table\n",
    "from Analysis import q1_stentence_length\n",
    "from Analysis import q2_sentence_cleaning\n",
    "from Analysis import q2_part_of_speech\n",
    "from Analysis import q3_keyword_frequency\n",
    "\n",
    "# Set display options for dataframes/plots\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "pd.options.display.max_colwidth=120\n",
    "\n",
    "folder = \"/Users/noel/Documents/GitHub/Projects_Selected/Frog Story Corpora/data_from_official_site\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Transform all transcripts into readable dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Extract information by researcher - Spanish\n",
    "\n",
    "b) Extract information by researcher - English\n",
    "\n",
    "c) Combine all the extracted information by languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Extract information by researcher - Spanish\n",
    "df_spa_agu = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"Spanish-Aguilar\"))\n",
    "df_spa_mia = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"Spanish-MiamiBiling\"))\n",
    "df_spa_orn = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"Spanish-Ornat\"))\n",
    "\n",
    "# b) Extract information by researcher - English\n",
    "df_eng_esc = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"English-ECSC\"))\n",
    "df_eng_mia1 = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"English-MiamiBiling\"))\n",
    "df_eng_mia2 = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"English-MiamiMono\"))\n",
    "df_eng_wol = raw_chat_to_dataframe(data_folder_raw=os.path.join(folder, \"English-WolfHemp\"))\n",
    "\n",
    "print(\"\\n Data preview: one data set for SPA - Aguilar:\")\n",
    "display(df_spa_agu.head(3))\n",
    "\n",
    "print(\"Data preview: one data set for ENG - ECSC:\")\n",
    "display(df_eng_esc.head(3))\n",
    "\n",
    "# c) Combine all the extracted information by languages \n",
    "df_all_spa = pd.concat([df_spa_agu, df_spa_mia, df_spa_orn])\n",
    "df_all_eng = pd.concat([df_eng_esc, df_eng_mia1, df_eng_mia2, df_eng_wol])\n",
    "\n",
    "print(\"Included researchers for all records:\")\n",
    "print(list(df_all_spa.lan_researcher.unique()))\n",
    "print(list(df_all_eng.lan_researcher.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Generate summary tables by languages - For combined records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Generate summary tables by languages - For combined records\n",
    "summary_all_spa = summary_table(df_all_spa, \"Spanish\")\n",
    "summary_all_eng = summary_table(df_all_eng, \"English\")\n",
    "\n",
    "print(\"=====\")\n",
    "print(\"TABLE 1: Summary statistics of data sources\")\n",
    "print(\"=====\")\n",
    "Tab1 = pd.concat([summary_all_spa, summary_all_eng], axis=1)\n",
    "Tab1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Random Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Randomly sample 100 records from each combined record\n",
    "\n",
    "b) Generate summary tables by languages - For sampled records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Randomly sample 100 records from each combined record\n",
    "randomness = 2023\n",
    "df_sample_spa = df_all_spa.sample(100, random_state=randomness).reset_index(drop=True)\n",
    "df_sample_eng = df_all_eng.sample(100, random_state=randomness).reset_index(drop=True)\n",
    "\n",
    "### b) Generate a summary table for the sampled records by languages \n",
    "summary_records_sample_spa = summary_table(df_sample_spa, \"Spanish\")\n",
    "summary_records_sample_eng = summary_table(df_sample_eng, \"English\")\n",
    "\n",
    "print(\"=====\")\n",
    "print(\"TABLE 2: Summary statistics of samples\")\n",
    "print(\"=====\")\n",
    "Tab2 = pd.concat([summary_records_sample_spa, summary_records_sample_eng], axis=1)\n",
    "Tab2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RQ1: Average sentence length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the number of sentences for each child\n",
    "\n",
    "b) Calculate the average sentence length for each child\n",
    "\n",
    "c) Plot the distribution of average sentence length by languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Calculate the number of sentences for each child\n",
    "df_sample_spa[\"sentence_count\"] = df_sample_spa.text.apply(lambda x: q1_stentence_length(x)[0])\n",
    "df_sample_eng[\"sentence_count\"] = df_sample_eng.text.apply(lambda x: q1_stentence_length(x)[0])\n",
    "\n",
    "# b) Calculate the average sentence length for each child\n",
    "df_sample_spa[\"sentence_length_mean\"] = df_sample_spa.text.apply(lambda x: q1_stentence_length(x)[1])\n",
    "df_sample_eng[\"sentence_length_mean\"] = df_sample_eng.text.apply(lambda x: q1_stentence_length(x)[1])\n",
    "\n",
    "# c) Plot the distribution of average sentence length by language\n",
    "plot_length_distribution, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3), sharex=True, sharey=True)\n",
    "plot_length_distribution.subplots_adjust(top=0.85)\n",
    "plot_length_distribution.suptitle(\"Distribution of average sentence length (mean)\", fontsize=12, x=0.5, y=0.95)\n",
    "\n",
    "sns.histplot(ax=axes[0], data=df_sample_spa, x=\"sentence_length_mean\", bins=10, color=\"blue\")\n",
    "sns.histplot(ax=axes[1], data=df_sample_eng, x=\"sentence_length_mean\", bins=10, color=\"orange\")\n",
    "\n",
    "# Modify the plot - Change the xy labels\n",
    "axes[0].set_xlabel(\"Spanish\"); axes[1].set_xlabel(\"English\")\n",
    "axes[0].set_ylabel(\"Number of interviewees\")\n",
    "\n",
    "# Modify the plot - Add the line of Mean\n",
    "mean_spa = round(df_sample_spa.sentence_length_mean.mean(), 1)\n",
    "mean_eng = round(df_sample_eng.sentence_length_mean.mean(), 1)\n",
    "\n",
    "axes[0].axhline(y = mean_spa, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "axes[1].axhline(y = mean_eng, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "axes[0].annotate(text = f\"Mean= {mean_spa}\", xy=(13, mean_spa+1))\n",
    "axes[1].annotate(text = f\"Mean= {mean_eng}\", xy=(13, mean_eng-3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Conduct T-test for the differences between the two languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### d) Conduct T-test for the differences \n",
    "length_ttest = stats.ttest_ind(\n",
    "    df_sample_spa[\"sentence_length_mean\"],\n",
    "    df_sample_eng[\"sentence_length_mean\"], \n",
    ")\n",
    "\n",
    "print(\"=====\")\n",
    "print(\"TABLE 3: Average sentence length by language\")\n",
    "print(\"=====\")\n",
    "\n",
    "Tab3 = pd.DataFrame({\n",
    "    \"Language\": [\"Spanish\", \"English\"],\n",
    "    \"Sample size\": [100, 100],\n",
    "    \"Mean\": [mean_spa, mean_eng],\n",
    "    \"Std\": [\n",
    "        round(df_sample_spa.sentence_length_mean.std(), 1), \n",
    "        round(df_sample_eng.sentence_length_mean.std(), 1)]\n",
    "})\n",
    "\n",
    "Tab3[\"Mean (± Std)\"] = Tab3['Mean'].astype(str) +\" (± \" + Tab3['Std'].astype(str) + \")\"\n",
    "Tab3[\"T statistic\"] = round(length_ttest.statistic, 1)\n",
    "Tab3[\"P value\"] = round(length_ttest.pvalue, 1)\n",
    "\n",
    "Tab3.at[1, \"T statistic\"] = \"N/A\" \n",
    "Tab3.at[1, \"P value\"] = \"N/A\"\n",
    "\n",
    "Tab3 = Tab3[[\"Language\", \"Sample size\", \"Mean (± Std)\", \"T statistic\", \"P value\"]]\n",
    "Tab3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) RQ2: Most frequently used part of speech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the language model from Spacy\n",
    "\n",
    "b) Clean the paragraphs \n",
    "\n",
    "c) Match the word tokens with POS and count the corresponding occurences\n",
    "\n",
    "d) Transform the obtained occurences into readable dataframe and calculate the % \n",
    "\n",
    "e) Plot the distribution for the occurences by languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load the language model from Spacy\n",
    "model_spa = spacy.load('es_dep_news_trf')\n",
    "model_eng = spacy.load('en_core_web_trf')\n",
    "\n",
    "# b) Clean the sentences \n",
    "clean_spa = q2_sentence_cleaning(df_dample = df_sample_spa, spa_or_eng_model = model_spa)\n",
    "clean_eng = q2_sentence_cleaning(df_dample = df_sample_eng, spa_or_eng_model = model_eng)\n",
    "\n",
    "# c) Match the word tokens with POS and count the corresponding occurences\n",
    "pos_spa = q2_part_of_speech(cleaned_sentences_list = clean_spa, spa_or_eng_model = model_spa)\n",
    "pos_eng = q2_part_of_speech(cleaned_sentences_list = clean_eng, spa_or_eng_model = model_eng)\n",
    "\n",
    "pos_match_spa, pos_count_spa = pos_spa[0], pos_spa[1]\n",
    "pos_match_eng, pos_count_eng = pos_eng[0], pos_eng[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Transform the obtained occurences into readable dataframe and calculate the % \n",
    "df_pos_spa = pd.DataFrame(list(pos_count_spa.items())).rename(columns={0:\"POS_spa\", 1:\"Count\"})\n",
    "df_pos_spa = df_pos_spa.sort_values(by = \"Count\", ascending=False).reset_index(drop = True)\n",
    "df_pos_spa[\"Count%\"] = round(df_pos_spa[\"Count\"] / df_pos_spa[\"Count\"].sum() * 100, 2)\n",
    "df_pos_spa[\"pos_count_order\"] = list(df_pos_spa.index + 1)\n",
    "\n",
    "df_pos_eng = pd.DataFrame(list(pos_count_eng.items())).rename(columns={0:\"POS_eng\", 1:\"Count\"})\n",
    "df_pos_eng = df_pos_eng.sort_values(by = \"Count\", ascending=False).reset_index(drop = True)\n",
    "df_pos_eng[\"Count%\"] = round(df_pos_eng[\"Count\"] / df_pos_eng[\"Count\"].sum() * 100, 2)\n",
    "df_pos_eng[\"pos_count_order\"] = list(df_pos_eng.index + 1)\n",
    "\n",
    "# d) Plot the distribution for the occurences by languages\n",
    "plot_pos_distribution, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharey=True)\n",
    "plot_pos_distribution.suptitle(\"Distribution of part of speech (occurences)\", fontsize=12, x=0.5, y=0.92)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_pos_spa, x=\"POS_spa\", y=\"Count%\", color='blue')\n",
    "sns.barplot(ax=axes[1], data=df_pos_eng, x=\"POS_eng\", y=\"Count%\", color='orange')\n",
    "\n",
    "# Modify the plot - Change the xy labels\n",
    "axes[0].set_xlabel(\"\"), axes[0].set_ylabel(\"Spanish (%)\")\n",
    "axes[1].set_xlabel(\"\"), axes[1].set_ylabel(\"English (%)\")\n",
    "\n",
    "axes[0].set_xticklabels(labels=list(df_pos_spa.POS_spa), rotation=20)\n",
    "axes[1].set_xticklabels(labels=list(df_pos_eng.POS_eng), rotation=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) RQ3: Usage frequenies of interested keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the language model from Spacy\n",
    "\n",
    "b) Clean the paragraphs \n",
    "\n",
    "c) Count the occurences of each interested keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load the language model from Spacy\n",
    "model_spa = spacy.load('es_dep_news_trf')\n",
    "model_eng = spacy.load('en_core_web_trf')\n",
    "\n",
    "# b) Clean the paragraphs \n",
    "clean_spa = q2_sentence_cleaning(df_dample = df_sample_spa, spa_or_eng_model = model_spa)\n",
    "clean_eng = q2_sentence_cleaning(df_dample = df_sample_eng, spa_or_eng_model = model_eng)\n",
    "\n",
    "# c) Count the occurences of each interested keyword\n",
    "keywords_eng = [\"hi\", \"Noel\", \"hello\"]\n",
    "test = q3_keyword_frequency(clean_eng, keywords_eng)\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "528b2abc06046716902442f23a73ad52c1ccb8212ba2b3f53ef0e284a157a7a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
